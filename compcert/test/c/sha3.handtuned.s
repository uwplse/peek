# File generated by CompCert 2.4
# Command line: -fno-peeps -stdlib ../../runtime -dc -dclight -dasm -S -o sha3.handtuned.s sha3.c
	.section	.rodata
	.align	1
__stringlit_4:
	.ascii	"Keccak-224 Test Hash\000"
	.type	__stringlit_4, @object
	.size	__stringlit_4, . - __stringlit_4
	.section	.rodata
	.align	1
__stringlit_6:
	.ascii	"passed\000"
	.type	__stringlit_6, @object
	.size	__stringlit_6, . - __stringlit_6
	.section	.rodata
	.align	1
__stringlit_1:
	.ascii	"Keccak-512 Test Hash\000"
	.type	__stringlit_1, @object
	.size	__stringlit_1, . - __stringlit_1
	.section	.rodata
	.align	1
__stringlit_7:
	.ascii	"SHA-3 %d %s\012\000"
	.type	__stringlit_7, @object
	.size	__stringlit_7, . - __stringlit_7
	.section	.rodata
	.align	1
__stringlit_3:
	.ascii	"Keccak-256 Test Hash\000"
	.type	__stringlit_3, @object
	.size	__stringlit_3, . - __stringlit_3
	.section	.rodata
	.align	1
__stringlit_2:
	.ascii	"Keccak-384 Test Hash\000"
	.type	__stringlit_2, @object
	.size	__stringlit_2, . - __stringlit_2
	.section	.rodata
	.align	1
__stringlit_5:
	.ascii	"FAILED\000"
	.type	__stringlit_5, @object
	.size	__stringlit_5, . - __stringlit_5
	.section	.rodata
	.align	4
	.global	keccakf_rndc
keccakf_rndc:
	.quad	1
	.quad	32898
	.quad	-9223372036854742902
	.quad	-9223372034707259392
	.quad	32907
	.quad	2147483649
	.quad	-9223372034707259263
	.quad	-9223372036854743031
	.quad	138
	.quad	136
	.quad	2147516425
	.quad	2147483658
	.quad	2147516555
	.quad	-9223372036854775669
	.quad	-9223372036854742903
	.quad	-9223372036854743037
	.quad	-9223372036854743038
	.quad	-9223372036854775680
	.quad	32778
	.quad	-9223372034707292150
	.quad	-9223372034707259263
	.quad	-9223372036854742912
	.quad	2147483649
	.quad	-9223372034707259384
	.type	keccakf_rndc, @object
	.size	keccakf_rndc, . - keccakf_rndc
	.section	.rodata
	.align	4
	.global	keccakf_rotc
keccakf_rotc:
	.long	1
	.long	3
	.long	6
	.long	10
	.long	15
	.long	21
	.long	28
	.long	36
	.long	45
	.long	55
	.long	2
	.long	14
	.long	27
	.long	41
	.long	56
	.long	8
	.long	25
	.long	43
	.long	62
	.long	18
	.long	39
	.long	61
	.long	20
	.long	44
	.type	keccakf_rotc, @object
	.size	keccakf_rotc, . - keccakf_rotc
	.section	.rodata
	.align	4
	.global	keccakf_piln
keccakf_piln:
	.long	10
	.long	7
	.long	11
	.long	17
	.long	18
	.long	3
	.long	5
	.long	16
	.long	8
	.long	21
	.long	24
	.long	4
	.long	15
	.long	23
	.long	19
	.long	13
	.long	12
	.long	2
	.long	20
	.long	14
	.long	22
	.long	9
	.long	6
	.long	1
	.type	keccakf_piln, @object
	.size	keccakf_piln, . - keccakf_piln
	.text
	.align	16
	.globl keccakf
keccakf:
	.cfi_startproc
	subl	$84, %esp
	.cfi_adjust_cfa_offset	84
	leal	88(%esp), %edx
	movl	%edx, 0(%esp)
	movl	%ebx, 4(%esp)
	movl	%esi, 8(%esp)
	movl	%edi, 12(%esp)
	movl	%ebp, 16(%esp)
	movl	0(%edx), %edi
	xorl	%eax, %eax
	movl	%eax, 32(%esp)
.L100:
	movl	0(%edi), %edx
	movl	4(%edi), %ebx
	movl	40(%edi), %ecx
	movl	44(%edi), %eax
	movl	%ebx, %ebp
	xorl	%eax, %ebp
	movl	%edx, %eax
	xorl	%ecx, %eax
	movl	80(%edi), %ecx
	movl	84(%edi), %esi
	xorl	%esi, %ebp
	xorl	%ecx, %eax
	movl	120(%edi), %ecx
	movl	124(%edi), %esi
	xorl	%esi, %ebp
	xorl	%ecx, %eax
	movl	160(%edi), %esi
	movl	164(%edi), %ecx
	xorl	%ecx, %ebp
	xorl	%esi, %eax
	movl	%eax, 40(%esp)
	movl	%ebp, 44(%esp)
//1.

  	movsd	8(%edi), %xmm0
	movsd	48(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	88(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	128(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	168(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	%xmm0, 48(%esp)
//2.

	movsd	16(%edi), %xmm0
	movsd	56(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	96(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	136(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	176(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	%xmm0, 56(%esp)
//3
	movsd	24(%edi), %xmm0
	movsd	64(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	104(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	144(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	184(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	%xmm0, 64(%esp)
//4

	movsd	32(%edi), %xmm0
	movsd	72(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	112(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	152(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	192(%edi), %xmm1
	xorpd	%xmm1, %xmm0
	movsd	%xmm0, 72(%esp)
	//5 this chunk looks kinda nasty
	//maybe you could vectorize it?
	movl	72(%esp), %ebp
	movl	76(%esp), %esi
	movl	48(%esp), %eax
	movl	%eax, 24(%esp)
	movl	52(%esp), %eax
	movl	%eax, 28(%esp)
	leal	0(,%eax,2), %eax
	movl	%eax, %ecx
	movl	24(%esp), %eax
	shrl	$31, %eax
	orl	%eax, %ecx
	movl	%ecx, %eax
	movl	24(%esp), %ecx
	leal	0(,%ecx,2), %ecx
	movl	%eax, 24(%esp)
	movl	28(%esp), %eax
	shrl	$31, %eax
	orl	%eax, %ecx
	movl	24(%esp), %eax
	xorl	%eax, %esi
	xorl	%ecx, %ebp
	xorl	%esi, %ebx
	xorl	%ebp, %edx
	movl	%edx, 0(%edi)
	movl	%ebx, 4(%edi)
	//6 %esi and %ebp are live in
	//orig used %esi and %ebp
	//now copy into %xmm0

	movd	%ebp, %xmm0
	movd	%esi, %xmm1
	pslldq	$4, %xmm1
	orpd	%xmm0, %xmm1
	//now vectorize
	movsd	40(%edi), %xmm2
	xorpd	%xmm1, %xmm2
	movsd	%xmm2, 40(%edi)
	movsd	80(%edi), %xmm2
	xorpd	%xmm1, %xmm2
	movsd	%xmm2, 80(%edi)
	movsd	120(%edi), %xmm2
	xorpd	%xmm1, %xmm2
	movsd	%xmm2, 120(%edi)
	movsd	160(%edi), %xmm2
	xorpd	%xmm1, %xmm2
	movsd	%xmm2, 160(%edi)

//7
	movl	40(%esp), %esi
	movl	44(%esp), %ebp
	movl	56(%esp), %ecx
	movl	60(%esp), %edx
	leal	0(,%edx,2), %ebx
	movl	%ecx, %eax
	shrl	$31, %eax
	orl	%eax, %ebx
	leal	0(,%ecx,2), %ecx
	shrl	$31, %edx
	orl	%edx, %ecx
	xorl	%ebx, %ebp
	xorl	%ecx, %esi
	
	movl	8(%edi), %eax
	movl	12(%edi), %ecx
	xorl	%ebp, %ecx
	xorl	%esi, %eax
	movl	%eax, 8(%edi)
	movl	%ecx, 12(%edi)

	movl	48(%edi), %edx
	movl	52(%edi), %eax
	xorl	%ebp, %eax
	xorl	%esi, %edx
	movl	%edx, 48(%edi)
	movl	%eax, 52(%edi)
	movl	88(%edi), %ecx
	movl	92(%edi), %edx
	xorl	%ebp, %edx
	xorl	%esi, %ecx
	movl	%ecx, 88(%edi)
	movl	%edx, 92(%edi)
	movl	128(%edi), %edx
	movl	132(%edi), %eax
	xorl	%ebp, %eax
	xorl	%esi, %edx
	movl	%edx, 128(%edi)
	movl	%eax, 132(%edi)
	movl	168(%edi), %eax
	movl	172(%edi), %ecx
	xorl	%ebp, %ecx
	xorl	%esi, %eax
	movl	%eax, 168(%edi)
	movl	%ecx, 172(%edi)

	movl	48(%esp), %ebx
	movl	52(%esp), %esi
	movl	64(%esp), %eax
	movl	68(%esp), %ecx
	leal	0(,%ecx,2), %ebp
	movl	%eax, %edx
	shrl	$31, %edx
	orl	%edx, %ebp
	leal	0(,%eax,2), %edx
	shrl	$31, %ecx
	orl	%ecx, %edx
	xorl	%ebp, %esi
	xorl	%edx, %ebx
	movl	16(%edi), %eax
	movl	20(%edi), %ecx
	xorl	%esi, %ecx
	xorl	%ebx, %eax
	movl	%eax, 16(%edi)
	movl	%ecx, 20(%edi)
	movl	56(%edi), %eax
	movl	60(%edi), %ecx
	xorl	%esi, %ecx
	xorl	%ebx, %eax
	movl	%eax, 56(%edi)
	movl	%ecx, 60(%edi)
	movl	96(%edi), %ecx
	movl	100(%edi), %edx
	xorl	%esi, %edx
	xorl	%ebx, %ecx
	movl	%ecx, 96(%edi)
	movl	%edx, 100(%edi)
	movl	136(%edi), %eax
	movl	140(%edi), %ecx
	xorl	%esi, %ecx
	xorl	%ebx, %eax
	movl	%eax, 136(%edi)
	movl	%ecx, 140(%edi)
	movl	176(%edi), %ecx
	movl	180(%edi), %eax
	xorl	%esi, %eax
	xorl	%ebx, %ecx
	movl	%ecx, 176(%edi)
	movl	%eax, 180(%edi)

	movl	56(%esp), %edx
	movl	60(%esp), %eax
	movl	72(%esp), %ebx
	movl	76(%esp), %esi
	leal	0(,%esi,2), %ecx
	movl	%ebx, %ebp
	shrl	$31, %ebp
	orl	%ebp, %ecx
	leal	0(,%ebx,2), %ebx
	shrl	$31, %esi
	orl	%esi, %ebx
	xorl	%ecx, %eax
	xorl	%ebx, %edx
	movl	24(%edi), %ecx
	movl	28(%edi), %ebx
	xorl	%eax, %ebx
	xorl	%edx, %ecx
	movl	%ecx, 24(%edi)
	movl	%ebx, 28(%edi)
	movl	64(%edi), %ecx
	movl	68(%edi), %ebx
	xorl	%eax, %ebx
	xorl	%edx, %ecx
	movl	%ecx, 64(%edi)
	movl	%ebx, 68(%edi)
	movl	104(%edi), %ecx
	movl	108(%edi), %ebx
	xorl	%eax, %ebx
	xorl	%edx, %ecx
	movl	%ecx, 104(%edi)
	movl	%ebx, 108(%edi)
	movl	144(%edi), %ecx
	movl	148(%edi), %ebx
	xorl	%eax, %ebx
	xorl	%edx, %ecx
	movl	%ecx, 144(%edi)
	movl	%ebx, 148(%edi)
	movl	184(%edi), %ecx
	movl	188(%edi), %ebx
	xorl	%eax, %ebx
	xorl	%edx, %ecx
	movl	%ecx, 184(%edi)
	movl	%ebx, 188(%edi)

	movl	64(%esp), %ecx
	movl	68(%esp), %eax
	movl	40(%esp), %esi
	movl	44(%esp), %ebp
	leal	0(,%ebp,2), %edx
	movl	%esi, %ebx
	shrl	$31, %ebx
	orl	%ebx, %edx
	leal	0(,%esi,2), %ebx
	shrl	$31, %ebp
	orl	%ebp, %ebx
	xorl	%edx, %eax
	xorl	%ebx, %ecx
	movl	32(%edi), %edx
	movl	36(%edi), %ebx
	xorl	%eax, %ebx
	xorl	%ecx, %edx
	movl	%edx, 32(%edi)
	movl	%ebx, 36(%edi)
	movl	72(%edi), %edx
	movl	76(%edi), %ebx
	xorl	%eax, %ebx
	xorl	%ecx, %edx
	movl	%edx, 72(%edi)
	movl	%ebx, 76(%edi)
	movl	112(%edi), %ebx
	movl	116(%edi), %edx
	xorl	%eax, %edx
	xorl	%ecx, %ebx
	movl	%ebx, 112(%edi)
	movl	%edx, 116(%edi)
	movl	152(%edi), %ebx
	movl	156(%edi), %edx
	xorl	%eax, %edx
	xorl	%ecx, %ebx
	movl	%ebx, 152(%edi)
	movl	%edx, 156(%edi)
	movl	192(%edi), %edx
	movl	196(%edi), %ebx
	xorl	%eax, %ebx
	xorl	%ecx, %edx
	movl	%edx, 192(%edi)
	movl	%ebx, 196(%edi)

	movl	8(%edi), %eax
	movl	12(%edi), %edx
	movl	80(%edi), %ecx
	movl	84(%edi), %esi
	movl	%ecx, 40(%esp)
	movl	%esi, 44(%esp)
	leal	0(,%edx,2), %ebp
	movl	%eax, %ebx
	shrl	$31, %ebx
	orl	%ebx, %ebp
	leal	0(,%eax,2), %eax
	shrl	$31, %edx
	orl	%edx, %eax
	movl	%eax, 80(%edi)
	movl	%ebp, 84(%edi)
	movl	%esi, %edx
	movl	56(%edi), %esi
	movl	60(%edi), %ebp
	movl	%esi, 40(%esp)
	movl	%ebp, 44(%esp)
	movl	%edx, %eax
	leal	0(,%eax,8), %ebx
	movl	%ecx, %edx
	shrl	$29, %edx
	orl	%edx, %ebx
	leal	0(,%ecx,8), %ecx
	shrl	$29, %eax
	orl	%eax, %ecx
	movl	%ecx, 56(%edi)
	movl	%ebx, 60(%edi)
	movl	88(%edi), %eax
	movl	92(%edi), %ecx
	movl	%eax, 40(%esp)
	movl	%ecx, 44(%esp)
	movl	%ebp, %edx
	shldl	$6, %esi, %edx
	sall	$6, %esi
	shrl	$26, %ebp
	orl	%ebp, %esi
	movl	%esi, 88(%edi)
	movl	%edx, 92(%edi)
	movl	136(%edi), %edx
	movl	140(%edi), %ebx
	movl	%edx, 40(%esp)
	movl	%ebx, 44(%esp)
	movl	%ecx, %esi
	shldl	$10, %eax, %esi
	sall	$10, %eax
	shrl	$22, %ecx
	orl	%ecx, %eax
	movl	%eax, 136(%edi)
	movl	%esi, 140(%edi)
	movl	144(%edi), %ecx
	movl	148(%edi), %eax
	movl	%ecx, 40(%esp)
	movl	%eax, 44(%esp)
	movl	%ebx, %esi
	shldl	$15, %edx, %esi
	sall	$15, %edx
	shrl	$17, %ebx
	orl	%ebx, %edx
	movl	%edx, 144(%edi)
	movl	%esi, 148(%edi)
	movl	24(%edi), %edx
	movl	28(%edi), %ebx
	movl	%edx, 40(%esp)
	movl	%ebx, 44(%esp)
	movl	%eax, %esi
	shldl	$21, %ecx, %esi
	sall	$21, %ecx
	shrl	$11, %eax
	orl	%eax, %ecx
	movl	%ecx, 24(%edi)
	movl	%esi, 28(%edi)
	movl	40(%edi), %ecx
	movl	44(%edi), %eax
	movl	%ecx, 40(%esp)
	movl	%eax, 44(%esp)
	movl	%ebx, %esi
	shldl	$28, %edx, %esi
	sall	$28, %edx
	shrl	$4, %ebx
	orl	%ebx, %edx
	movl	%edx, 40(%edi)
	movl	%esi, 44(%edi)
	movl	128(%edi), %ebx
	movl	132(%edi), %edx
	movl	%ebx, 40(%esp)
	movl	%edx, 44(%esp)
	movl	%eax, %esi
	shrl	$28, %esi
	shldl	$4, %ecx, %eax
	sall	$4, %ecx
	orl	%esi, %ecx
	movl	%eax, 128(%edi)
	movl	%ecx, 132(%edi)
	movl	64(%edi), %eax
	movl	68(%edi), %esi
	movl	%eax, 40(%esp)
	movl	%esi, 44(%esp)
	movl	%edx, %ecx
	shrl	$19, %ecx
	shldl	$13, %ebx, %edx
	sall	$13, %ebx
	orl	%ecx, %ebx
	movl	%edx, 64(%edi)
	movl	%ebx, 68(%edi)
	movl	%eax, %edx
	movl	168(%edi), %eax
	movl	172(%edi), %ecx
	movl	%eax, 40(%esp)
	movl	%ecx, 44(%esp)
	movl	%esi, %ebx
	shrl	$9, %ebx
	shldl	$23, %edx, %esi
	sall	$23, %edx
	orl	%ebx, %edx
	movl	%esi, 168(%edi)
	movl	%edx, 172(%edi)
	movl	192(%edi), %ebx
	movl	196(%edi), %esi
	movl	%ebx, 40(%esp)
	movl	%esi, 44(%esp)
	leal	0(,%ecx,4), %ebp
	movl	%eax, %edx
	shrl	$30, %edx
	orl	%edx, %ebp
	leal	0(,%eax,4), %edx
	shrl	$30, %ecx
	orl	%ecx, %edx
	movl	%edx, 192(%edi)
	movl	%ebp, 196(%edi)
	movl	32(%edi), %edx
	movl	36(%edi), %eax
	movl	%edx, 40(%esp)
	movl	%eax, 44(%esp)
	movl	%esi, %ecx
	shldl	$14, %ebx, %ecx
	sall	$14, %ebx
	shrl	$18, %esi
	orl	%esi, %ebx
	movl	%ebx, 32(%edi)
	movl	%ecx, 36(%edi)
	movl	120(%edi), %ebx
	movl	124(%edi), %esi
	movl	%ebx, 40(%esp)
	movl	%esi, 44(%esp)
	movl	%eax, %ecx
	shldl	$27, %edx, %ecx
	sall	$27, %edx
	shrl	$5, %eax
	orl	%eax, %edx
	movl	%edx, 120(%edi)
	movl	%ecx, 124(%edi)
	movl	184(%edi), %edx
	movl	188(%edi), %ecx
	movl	%edx, 40(%esp)
	movl	%ecx, 44(%esp)
	movl	%esi, %eax
	shrl	$23, %eax
	shldl	$9, %ebx, %esi
	sall	$9, %ebx
	orl	%eax, %ebx
	movl	%esi, 184(%edi)
	movl	%ebx, 188(%edi)
	movl	152(%edi), %esi
	movl	156(%edi), %ebx
	movl	%esi, 40(%esp)
	movl	%ebx, 44(%esp)
	movl	%ecx, %eax
	shrl	$8, %eax
	shldl	$24, %edx, %ecx
	sall	$24, %edx
	orl	%eax, %edx
	movl	%ecx, 152(%edi)
	movl	%edx, 156(%edi)
	movl	104(%edi), %edx
	movl	108(%edi), %eax
	movl	%edx, 40(%esp)
	movl	%eax, 44(%esp)
	movl	%ebx, %ecx
	shldl	$8, %esi, %ecx
	sall	$8, %esi
	shrl	$24, %ebx
	orl	%ebx, %esi
	movl	%esi, 104(%edi)
	movl	%ecx, 108(%edi)
	movl	96(%edi), %ecx
	movl	100(%edi), %ebx
	movl	%ecx, 40(%esp)
	movl	%ebx, 44(%esp)
	movl	%eax, %esi
	shldl	$25, %edx, %esi
	sall	$25, %edx
	shrl	$7, %eax
	orl	%eax, %edx
	movl	%edx, 96(%edi)
	movl	%esi, 100(%edi)
	movl	16(%edi), %edx
	movl	20(%edi), %eax
	movl	%edx, 40(%esp)
	movl	%eax, 44(%esp)
	movl	%ebx, %esi
	shrl	$21, %esi
	shldl	$11, %ecx, %ebx
	sall	$11, %ecx
	orl	%esi, %ecx
	movl	%ebx, 16(%edi)
	movl	%ecx, 20(%edi)
	movl	160(%edi), %ebx
	movl	164(%edi), %esi
	movl	%ebx, 40(%esp)
	movl	%esi, 44(%esp)
	movl	%eax, %ecx
	shrl	$2, %ecx
	shldl	$30, %edx, %eax
	sall	$30, %edx
	orl	%ecx, %edx
	movl	%eax, 160(%edi)
	movl	%edx, 164(%edi)
	movl	112(%edi), %ecx
	movl	116(%edi), %edx
	movl	%ecx, 40(%esp)
	movl	%edx, 44(%esp)
	movl	%esi, %eax
	shldl	$18, %ebx, %eax
	sall	$18, %ebx
	shrl	$14, %esi
	orl	%esi, %ebx
	movl	%ebx, 112(%edi)
	movl	%eax, 116(%edi)
	movl	176(%edi), %ebx
	movl	180(%edi), %esi
	movl	%ebx, 40(%esp)
	movl	%esi, 44(%esp)
	movl	%edx, %eax
	shrl	$25, %eax
	shldl	$7, %ecx, %edx
	sall	$7, %ecx
	orl	%eax, %ecx
	movl	%edx, 176(%edi)
	movl	%ecx, 180(%edi)
	movl	72(%edi), %eax
	movl	76(%edi), %ecx
	movl	%eax, 40(%esp)
	movl	%ecx, 44(%esp)
	movl	%esi, %edx
	shrl	$3, %edx
	shldl	$29, %ebx, %esi
	sall	$29, %ebx
	orl	%edx, %ebx
	movl	%esi, 72(%edi)
	movl	%ebx, 76(%edi)
	movl	48(%edi), %esi
	movl	52(%edi), %ebx
	movl	%esi, 40(%esp)
	movl	%ebx, 44(%esp)
	movl	%ecx, %edx
	shldl	$20, %eax, %edx
	sall	$20, %eax
	shrl	$12, %ecx
	orl	%ecx, %eax
	movl	%eax, 48(%edi)
	movl	%edx, 52(%edi)
	movl	8(%edi), %edx
	movl	12(%edi), %ecx
	movl	%edx, 40(%esp)
	movl	%ecx, 44(%esp)
	movl	%ebx, %ecx
	shrl	$20, %ecx
	shldl	$12, %esi, %ebx
	sall	$12, %esi
	orl	%ecx, %esi
	movl	%ebx, 8(%edi)
	movl	%esi, 12(%edi)
	xorl	%edx, %edx
	movl	%edx, 24(%esp)
.L101:
	movl	24(%esp), %ecx
	movl	0(%edi,%ecx,8), %eax
	movl	4(%edi,%ecx,8), %ecx
	movl	%eax, 40(%esp)
	movl	%ecx, 44(%esp)
	movl	24(%esp), %edx
	movl	8(%edi,%edx,8), %ebx
	movl	12(%edi,%edx,8), %edx
	movl	%ebx, 48(%esp)
	movl	%edx, 52(%esp)
	movl	24(%esp), %edx
	movl	16(%edi,%edx,8), %ebx
	movl	20(%edi,%edx,8), %edx
	movl	%ebx, 56(%esp)
	movl	%edx, 60(%esp)
	movl	24(%esp), %edx
	movl	24(%edi,%edx,8), %ebx
	movl	28(%edi,%edx,8), %edx
	movl	%ebx, 64(%esp)
	movl	%edx, 68(%esp)
	movl	24(%esp), %edx
	movl	32(%edi,%edx,8), %ebx
	movl	36(%edi,%edx,8), %edx
	movl	%ebx, 72(%esp)
	movl	%edx, 76(%esp)
	
//1.
	movsd	40(%esp), %xmm2
	movsd	48(%esp), %xmm0
	movsd	56(%esp), %xmm1
	andnpd 	%xmm1, %xmm0
	xorpd	%xmm0, %xmm2
	movl 	24(%esp), %edx
	movsd	%xmm2, 0(%edi,%edx,8)
//2.
	movsd	8(%edi,%edx,8), %xmm0
	movsd	56(%esp), %xmm1
	movsd	64(%esp), %xmm2
	andnpd	%xmm2, %xmm1
	xorpd	%xmm1, %xmm0
	movl	24(%esp), %edx
	movsd	%xmm0, 8(%edi,%edx,8)
//3.	

	movsd	16(%edi,%edx,8), %xmm0
	movsd	64(%esp), %xmm1
	movsd	72(%esp), %xmm2
	andnpd	%xmm2,%xmm1
	xorpd	%xmm1,%xmm0
	movl	24(%esp), %edx
	movsd	%xmm0, 16(%edi,%edx,8)
	
//4.	

	movsd	24(%edi,%edx,8), %xmm0
	movsd	72(%esp), %xmm1
	movsd	40(%esp), %xmm2
	andnpd	%xmm2,%xmm1
	xorpd	%xmm1,%xmm0
	movl	24(%esp), %edx
	movsd	%xmm0, 24(%edi,%edx,8)
//5.

	movsd	32(%edi,%edx,8), %xmm0
	movsd	40(%esp), %xmm1
	movsd	48(%esp), %xmm2
	andnpd	%xmm2,%xmm1
	xorpd	%xmm1,%xmm0
	movl	24(%esp), %eax
	movsd	%xmm0, 32(%edi,%eax,8)
//end
	leal	5(%eax), %edx
	movl	%edx, 24(%esp)
	cmpl	$25, %edx
	jl	.L101
	movl	0(%edi), %eax
	movl	4(%edi), %edx
	movl	32(%esp), %ecx
	movl	keccakf_rndc(,%ecx,8), %ebx
	movl	(keccakf_rndc + 4)(,%ecx,8), %ecx
	xorl	%ecx, %edx
	xorl	%ebx, %eax
	movl	%eax, 0(%edi)
	movl	%edx, 4(%edi)
	movl	32(%esp), %eax
	leal	1(%eax), %ecx
	movl	%ecx, 32(%esp)
	cmpl	$24, %ecx
	jl	.L100
	movl	4(%esp), %ebx
	movl	8(%esp), %esi
	movl	12(%esp), %edi
	movl	16(%esp), %ebp
	addl	$84, %esp
	ret
	.cfi_endproc
	.type	keccakf, @function
	.size	keccakf, . - keccakf
	.text
	.align	16
	.globl keccak
keccak:
	.cfi_startproc
	subl	$404, %esp
	.cfi_adjust_cfa_offset	404
	leal	408(%esp), %edx
	movl	%edx, 12(%esp)
	movl	%ebx, 16(%esp)
	movl	%esi, 20(%esp)
	movl	%edi, 24(%esp)
	movl	%ebp, 28(%esp)
	movl	0(%edx), %esi
	movl	4(%edx), %edx
	movl	%edx, 32(%esp)
	movl	12(%esp), %edx
	movl	8(%edx), %edx
	movl	%edx, 48(%esp)
	movl	12(%esp), %edx
	movl	12(%edx), %edx
	movl	%edx, 44(%esp)
	movl	$200, %ebx
	movl	44(%esp), %eax
	leal	0(,%eax,2), %eax
	subl	%eax, %ebx
	movl	%ebx, %eax
	testl	%eax, %eax
	leal	7(%eax), %ecx
	cmovl	%ecx, %eax
	sarl	$3, %eax
	movl	%eax, 40(%esp)
	leal	200(%esp), %ecx
	xorl	%eax, %eax
	movl	$200, %edx
	movl	%ecx, 0(%esp)
	movl	%eax, 4(%esp)
	movl	%edx, 8(%esp)
	call	memset
.L102:
	movl	32(%esp), %edx
	cmpl	%ebx, %edx
	jl	.L103
	xorl	%edi, %edi
.L104:
	movl	40(%esp), %ecx
	cmpl	%ecx, %edi
	jge	.L105
	movzbl	0(%esi,%edi,8), %eax
	movzbl	1(%esi,%edi,8), %ecx
	sall	$8, %ecx
	orl	%ecx, %eax
	movzbl	2(%esi,%edi,8), %edx
	sall	$16, %edx
	orl	%edx, %eax
	movzbl	3(%esi,%edi,8), %ecx
	sall	$24, %ecx
	orl	%ecx, %eax
	movzbl	4(%esi,%edi,8), %ebp
	movzbl	5(%esi,%edi,8), %ecx
	sall	$8, %ecx
	orl	%ecx, %ebp
	movzbl	6(%esi,%edi,8), %ecx
	sall	$16, %ecx
	orl	%ecx, %ebp
	movzbl	7(%esi,%edi,8), %edx
	sall	$24, %edx
	orl	%edx, %ebp
	leal	200(%esp), %ecx
	movl	%ecx, 36(%esp)
	leal	200(%esp), %ecx
	movl	0(%ecx,%edi,8), %edx
	movl	4(%ecx,%edi,8), %ecx
	xorl	%ebp, %ecx
	xorl	%eax, %edx
	movl	36(%esp), %eax
	movl	%edx, 0(%eax,%edi,8)
	movl	%ecx, 4(%eax,%edi,8)
	leal	1(%edi), %edi
	jmp	.L104
.L105:
	leal	200(%esp), %ecx
	movl	%ecx, 0(%esp)
	call	keccakf
	movl	32(%esp), %eax
	subl	%ebx, %eax
	movl	%eax, 32(%esp)
	leal	0(%esi,%ebx,1), %esi
	jmp	.L102
.L103:
	leal	56(%esp), %eax
	movl	%eax, 0(%esp)
	movl	%esi, 4(%esp)
	movl	32(%esp), %ecx
	movl	%ecx, 8(%esp)
	call	memcpy
	movl	32(%esp), %edx
	movl	%edx, %esi
	leal	1(%edx), %edx
	leal	56(%esp), %ecx
	movl	$1, %eax
	movb	%al, 0(%ecx,%esi,1)
	leal	56(%esp), %eax
	leal	0(%eax,%edx,1), %esi
	xorl	%ecx, %ecx
	movl	%ebx, %eax
	subl	%edx, %eax
	movl	%esi, 0(%esp)
	movl	%ecx, 4(%esp)
	movl	%eax, 8(%esp)
	call	memset
	leal	56(%esp), %edx
	leal	56(%esp), %eax
	movzbl	-1(%eax,%ebx,1), %ecx
	orl	$128, %ecx
	movb	%cl, -1(%edx,%ebx,1)
	xorl	%ebx, %ebx
.L106:
	movl	40(%esp), %ecx
	cmpl	%ecx, %ebx
	jge	.L107
	leal	56(%esp), %esi
	movzbl	0(%esi,%ebx,8), %edx
	movzbl	1(%esi,%ebx,8), %eax
	sall	$8, %eax
	orl	%eax, %edx
	movzbl	2(%esi,%ebx,8), %ecx
	sall	$16, %ecx
	orl	%ecx, %edx
	movzbl	3(%esi,%ebx,8), %eax
	sall	$24, %eax
	orl	%eax, %edx
	movzbl	4(%esi,%ebx,8), %ecx
	movzbl	5(%esi,%ebx,8), %eax
	sall	$8, %eax
	orl	%eax, %ecx
	movzbl	6(%esi,%ebx,8), %eax
	sall	$16, %eax
	orl	%eax, %ecx
	movzbl	7(%esi,%ebx,8), %eax
	sall	$24, %eax
	orl	%eax, %ecx
	leal	200(%esp), %eax
	leal	200(%esp), %edi
	movl	0(%edi,%ebx,8), %esi
	movl	4(%edi,%ebx,8), %edi
	xorl	%ecx, %edi
	xorl	%edx, %esi
	movl	%esi, 0(%eax,%ebx,8)
	movl	%edi, 4(%eax,%ebx,8)
	leal	1(%ebx), %ebx
	jmp	.L106
.L107:
	leal	200(%esp), %edx
	movl	%edx, 0(%esp)
	call	keccakf
	xorl	%ebp, %ebp
.L108:
	leal	56(%esp), %edi
	leal	200(%esp), %ecx
	movl	0(%ecx,%ebp,8), %ebx
	movl	4(%ecx,%ebp,8), %edx
	movb	%bl, 0(%edi,%ebp,8)
	movl	%edx, %esi
	shrl	$8, %esi
	movl	%edx, %eax
	shldl	$24, %ebx, %eax
	movb	%al, 1(%edi,%ebp,8)
	movl	%edx, %eax
	shrl	$16, %eax
	movl	%eax, 36(%esp)
	movl	%edx, %eax
	shldl	$16, %ebx, %eax
	movb	%al, 2(%edi,%ebp,8)
	movl	%edx, %eax
	shrl	$24, %eax
	movl	%eax, 32(%esp)
	movl	%edx, %ecx
	shldl	$8, %ebx, %ecx
	movb	%cl, 3(%edi,%ebp,8)
	movb	%dl, 4(%edi,%ebp,8)
	movl	%esi, %eax
	movb	%al, 5(%edi,%ebp,8)
	movl	36(%esp), %eax
	movb	%al, 6(%edi,%ebp,8)
	movl	32(%esp), %ecx
	movb	%cl, 7(%edi,%ebp,8)
	leal	1(%ebp), %ebp
	cmpl	$8, %ebp
	jl	.L108
	leal	56(%esp), %ecx
	movl	48(%esp), %edx
	movl	%edx, 0(%esp)
	movl	%ecx, 4(%esp)
	movl	44(%esp), %edx
	movl	%edx, 8(%esp)
	call	memcpy
	movl	16(%esp), %ebx
	movl	20(%esp), %esi
	movl	24(%esp), %edi
	movl	28(%esp), %ebp
	addl	$404, %esp
	ret
	.cfi_endproc
	.type	keccak, @function
	.size	keccak, . - keccak
	.data
	.align	4
	.global	testvec
testvec:
	.long	28
	.long	__stringlit_4
	.byte	48
	.byte	4
	.byte	91
	.byte	52
	.byte	148
	.byte	110
	.byte	27
	.byte	46
	.byte	9
	.byte	22
	.byte	19
	.byte	54
	.byte	47
	.byte	210
	.byte	42
	.byte	160
	.byte	142
	.byte	43
	.byte	234
	.byte	254
	.byte	197
	.byte	232
	.byte	218
	.byte	238
	.byte	66
	.byte	194
	.byte	230
	.byte	101
	.space	36
	.long	32
	.long	__stringlit_3
	.byte	168
	.byte	215
	.byte	27
	.byte	7
	.byte	244
	.byte	175
	.byte	38
	.byte	164
	.byte	255
	.byte	33
	.byte	2
	.byte	127
	.byte	98
	.byte	255
	.byte	96
	.byte	38
	.byte	127
	.byte	249
	.byte	85
	.byte	201
	.byte	99
	.byte	240
	.byte	66
	.byte	196
	.byte	109
	.byte	165
	.byte	46
	.byte	227
	.byte	207
	.byte	175
	.byte	61
	.byte	60
	.space	32
	.long	48
	.long	__stringlit_2
	.byte	226
	.byte	19
	.byte	253
	.byte	116
	.byte	175
	.byte	12
	.byte	95
	.byte	249
	.byte	27
	.byte	66
	.byte	60
	.byte	139
	.byte	206
	.byte	236
	.byte	215
	.byte	1
	.byte	248
	.byte	221
	.byte	100
	.byte	236
	.byte	24
	.byte	253
	.byte	111
	.byte	146
	.byte	96
	.byte	252
	.byte	158
	.byte	193
	.byte	237
	.byte	189
	.byte	34
	.byte	48
	.byte	166
	.byte	144
	.byte	134
	.byte	101
	.byte	188
	.byte	217
	.byte	251
	.byte	244
	.byte	26
	.byte	153
	.byte	161
	.byte	138
	.byte	125
	.byte	158
	.byte	68
	.byte	110
	.space	16
	.long	64
	.long	__stringlit_1
	.byte	150
	.byte	238
	.byte	71
	.byte	24
	.byte	220
	.byte	186
	.byte	60
	.byte	116
	.byte	97
	.byte	155
	.byte	161
	.byte	250
	.byte	127
	.byte	87
	.byte	223
	.byte	231
	.byte	118
	.byte	157
	.byte	63
	.byte	102
	.byte	152
	.byte	168
	.byte	179
	.byte	63
	.byte	161
	.byte	1
	.byte	131
	.byte	137
	.byte	112
	.byte	161
	.byte	49
	.byte	230
	.byte	33
	.byte	204
	.byte	253
	.byte	5
	.byte	254
	.byte	255
	.byte	188
	.byte	17
	.byte	128
	.byte	242
	.byte	99
	.byte	194
	.byte	127
	.byte	26
	.byte	218
	.byte	180
	.byte	96
	.byte	149
	.byte	214
	.byte	241
	.byte	37
	.byte	51
	.byte	20
	.byte	114
	.byte	75
	.byte	92
	.byte	191
	.byte	120
	.byte	40
	.byte	101
	.byte	142
	.byte	106
	.type	testvec, @object
	.size	testvec, . - testvec
	.local	data
	.comm	data, 100000, 1
	.text
	.align	16
	.globl main
main:
	.cfi_startproc
	subl	$100, %esp
	.cfi_adjust_cfa_offset	100
	leal	104(%esp), %edx
	movl	%edx, 16(%esp)
	movl	%ebx, 20(%esp)
	movl	%esi, 24(%esp)
	movl	%edi, 28(%esp)
	xorl	%ebx, %ebx
.L109:
	movl	%ebx, %ecx
	sall	$6, %ecx
	leal	0(%ecx,%ebx,8), %eax
	movl	(testvec + 4)(%eax), %edx
	movl	%edx, 0(%esp)
	call	strlen
	movl	%ebx, %edx
	sall	$6, %edx
	leal	0(%edx,%ebx,8), %ecx
	movl	(testvec + 4)(%ecx), %esi
	leal	32(%esp), %edx
	movl	testvec(%ecx), %ecx
	movl	%esi, 0(%esp)
	movl	%eax, 4(%esp)
	movl	%edx, 8(%esp)
	movl	%ecx, 12(%esp)
	call	keccak
	leal	32(%esp), %ecx
	movl	%ebx, %edx
	sall	$6, %edx
	leal	0(%edx,%ebx,8), %eax
	leal	(testvec + 8)(%eax), %edx
	movl	testvec(%eax), %esi
	movl	%ecx, 0(%esp)
	movl	%edx, 4(%esp)
	movl	%esi, 8(%esp)
	call	memcmp
	testl	%eax, %eax
	je	.L110
	leal	__stringlit_5, %ecx
	jmp	.L111
.L110:
	leal	__stringlit_6, %ecx
.L111:
	leal	__stringlit_7, %esi
	movl	%ebx, %eax
	sall	$6, %eax
	leal	0(%eax,%ebx,8), %eax
	movl	testvec(%eax), %edx
	leal	0(,%edx,8), %edx
	movl	%esi, 0(%esp)
	movl	%edx, 4(%esp)
	movl	%ecx, 8(%esp)
	call	printf
	leal	1(%ebx), %ebx
	cmpl	$4, %ebx
	jl	.L109
	xorl	%edx, %edx
.L112:
	movb	%dl, data(%edx)
	leal	1(%edx), %edx
	cmpl	$100000, %edx
	jl	.L112
	xorl	%ebx, %ebx
.L113:
	leal	data, %edi
	movl	$100000, %esi
	leal	32(%esp), %ecx
	movl	$64, %edx
	movl	%edi, 0(%esp)
	movl	%esi, 4(%esp)
	movl	%ecx, 8(%esp)
	movl	%edx, 12(%esp)
	call	keccak
	leal	1(%ebx), %ebx
	cmpl	$1000, %ebx
	jl	.L113
	xorl	%eax, %eax
	movl	20(%esp), %ebx
	movl	24(%esp), %esi
	movl	28(%esp), %edi
	addl	$100, %esp
	ret
	.cfi_endproc
	.type	main, @function
	.size	main, . - main
